{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyfJD5rTCZ6f",
        "colab_type": "code",
        "outputId": "27a149d4-50e9-44a9-f91b-ab7bfbedca27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Assignment 2\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cU0MLe5sjlfG",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jH_2JxU9jj5J",
        "colab": {}
      },
      "source": [
        "!wget -q http://apache.mirrors.hoobly.com/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFdbi-ubN5P_",
        "colab_type": "code",
        "outputId": "f5a0a24c-3564-417e-9c4e-9b839fe032ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.csv       mycsv.csv    spark-2.4.5-bin-hadoop2.7\t     task_1.csv\n",
            "drive\t       new2\t    spark-2.4.5-bin-hadoop2.7.tgz\n",
            "file.csv       new.csv\t    spark-2.4.5-bin-hadoop2.7.tgz.1\n",
            "find_text.csv  sample_data  spark-2.4.5-bin-hadoop2.7.tgz.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBT22YYrOOjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6i_oyDFOqXG",
        "colab_type": "code",
        "outputId": "ac667fe0-0696-4bfb-895f-9dc3bd883865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!ls "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.csv       mycsv.csv    spark-2.4.5-bin-hadoop2.7\t     task_1.csv\n",
            "drive\t       new2\t    spark-2.4.5-bin-hadoop2.7.tgz\n",
            "file.csv       new.csv\t    spark-2.4.5-bin-hadoop2.7.tgz.1\n",
            "find_text.csv  sample_data  spark-2.4.5-bin-hadoop2.7.tgz.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5w5KMo3O8aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P4gXyFyJpug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FY0mR4gKOla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"groupbyagg\").getOrCreate()\n",
        "from pyspark.sql import SQLContext    \n",
        "from pyspark.sql.types import DataType\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.types import IntegerType\n",
        "import pyspark.sql.functions as func\n",
        "import pandas as pd\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1CGvyhqRGCi",
        "colab_type": "code",
        "outputId": "26cc8abc-cf36-4e13-d6ab-3a456109554c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#reading and importing data to a Spark Dataframe\n",
        "df = spark.read.csv(\"data.csv\", inferSchema = True, header = True)\n",
        "#Filtering out columns for analysis\n",
        "df = df.select('id_str', 'tweet_created_at', 'user_verified', 'favorite_count', 'retweet_count', 'text_')\n",
        "#Step 1: Remove the records where “user_verified” is “FALSE”. \n",
        "df1 = df.where(df.user_verified==\"True\")\n",
        "df1.count()       \n",
        "\n",
        "# Number of records = 171797"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCF4TqChc1Wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaning the datetime column to extract day\n",
        "df2 = df1.withColumn('day', df['tweet_created_at'].substr(5,6))\n",
        "df2 = df2.drop(df2.tweet_created_at)\n",
        "df2 = df2.withColumnRenamed(\"day\",\"tweet_created_at\")\n",
        "#Step 2: For the remaining records (“user_verified” is “TRUE”), group by created date, and count the number of tweets for each date. \n",
        "data_counts = df2.groupby(\"tweet_created_at\").count()\n",
        "max_value = data_counts.select(func.max('count')).collect()[0][0]\n",
        "result = data_counts.filter(data_counts[\"count\"] == max_value)\n",
        "created_date = result.collect()[0][0]\n",
        "print(created_date)   #Jan 03\n",
        "df3 = df2[df2.tweet_created_at == created_date]\n",
        "#For the date with highest number of tweets calculate the sum\n",
        "#of “favorite_count” and “retweet_count” for each tweet on that day, and find out the text content\n",
        "#(“text_”) of the top 100 tweets with highest sum\n",
        "df3 = df3.withColumn(\"favorite_count\", df3[\"favorite_count\"].cast(IntegerType()))\n",
        "df3 = df3.withColumn(\"retweet_count\", df3[\"retweet_count\"].cast(IntegerType()))\n",
        "df3 = df3.withColumn('total_col', df3.favorite_count + df3.retweet_count).sort('total_col', ascending=False).limit(100)\n",
        "df3 = df3.select(\"text_\")\n",
        "df3.show(n=100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldSh5nu7TdbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a csv to export top 100 tweets from a Spark Dataframe to a csv\n",
        "df3 = df3.withColumn('word', func.explode(func.split(func.col('text_'), ' ')))\\\n",
        "    .groupBy('word')\\\n",
        "    .count()\\\n",
        "    .sort('count', ascending=False)\n",
        "df3.write.csv('task_1.csv',header= \"TRUE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J4kmu5BWLJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Task 2: Joining mytask.csv and Amazon data to a single dataframe \n",
        "#using spark and exporting the results. The joins were done on id_str\n",
        "text_df = spark.read.csv(\"find_text.csv\", inferSchema = True, header = True)\n",
        "text_df = text_df.select(\"id_str\")\n",
        "text_df.count()\n",
        "text_combined = text_df.join(df,\"id_str\",\"left\")\n",
        "text_combined = text_combined.select(\"id_str\",\"text_\")\n",
        "text_combined.show()\n",
        "#Convert to csv and export file\n",
        "text_combined.coalesce(1).write.option(\"header\", \"true\").csv(\"sample_file.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxnjKhHXRnQ8",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------"
      ]
    }
  ]
}